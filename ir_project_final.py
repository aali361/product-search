# -*- coding: utf-8 -*-
"""IR_Project_Final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pVMS8Fpf2Fised8pChBtBTrYgk-XmxqC
"""

from google.colab import drive
drive.mount('/content/drive')
!cp /content/drive/My\ Drive/Colab\ Notebooks/dataset.zip .
!cp /content/drive/My\ Drive/Colab\ Notebooks/vectors_item_treshold3.dat .
!cp /content/drive/My\ Drive/Colab\ Notebooks/vectors_item_treshold5.dat .
!cp /content/drive/My\ Drive/Colab\ Notebooks/vectors_item_treshold10.dat .
!cp /content/drive/My\ Drive/Colab\ Notebooks/vectors_token_treshold3.dat .
!cp /content/drive/My\ Drive/Colab\ Notebooks/vectors_token_treshold5.dat .
!cp /content/drive/My\ Drive/Colab\ Notebooks/vectors_token_treshold10.dat .

import pandas as pd
import numpy as np
# For creating edges
from collections import defaultdict

# For cosine similarity
from scipy import spatial

# For sorting temp
import operator, collections

# For NDCG
import math

# We get warning when we want to compute distance
import warnings
warnings.filterwarnings('ignore')

# Biparitie Graph
import networkx as nx
from networkx.algorithms import bipartite
import matplotlib.pyplot as plt

!unzip dataset.zip
!rm train-purchases.csv -f
!rm train-item-views.csv -f
!rm train-clicks.csv -f
!rm product-categories.csv -f
!rm products.csv -f
!rm dataset.zip -f
!rm -rf sample_data/

# Read a csv file and return sampled data
def getSampledData(file_name, frac=1):
  all_data = pd.read_csv(file_name, delimiter=';')
  all_data.columns = all_data.columns.str.replace(',','')
  all_data = all_data[pd.notnull(all_data['searchstring.tokens'])]
  all_data = all_data[pd.notnull(all_data['items'])]
  sampled_data = all_data.sample(frac= frac).reset_index(drop=True)
  
  return sampled_data, all_data

sampled_data, all_data = getSampledData('train-queries.csv')

sampled_data

def getNodesAndEdgesAndGraph(df):
  Bipartie = nx.Graph()
  tokens_ids = []
  items_ids = []
  edges = {}
  edges = defaultdict(lambda:0, edges)

  for key, tokens in enumerate(df['searchstring.tokens'][:]):
    items = df['items'][key]
    is_test = df['is.test'][key]
    tokens = tokens.split(',')
    items = items.split(',')
    Bipartie.add_nodes_from(tokens, bipartite=0)
    Bipartie.add_nodes_from(items, bipartite=1)
    for item in items:
      items_ids.append(int(item)+2000000)
    for token in tokens:
      tokens_ids.append(int(token)+1000000)
      for item in items:
        edges[(int(token)+1000000, int(item)+2000000, is_test)] += 1
        Bipartie.add_edge(token, item)
  
  return tokens_ids, items_ids, edges, Bipartie

tokens, items, edges, Bipartie = getNodesAndEdgesAndGraph(sampled_data)

degree_sequence=sorted(dict(nx.degree(Bipartie)).values(),reverse=True) 
plt.loglog(degree_sequence,marker='*')
plt.xlabel('Count')
plt.ylabel('Degree') 
plt.title('Amazon')
plt.figure(figsize=(15,8))
plt.show()

def limitEdges(edges, threshold= 2):
  from copy import copy
  temp_edges = copy(edges)
  for k, v in list(edges.items())[:]:
    if v < threshold:
      del temp_edges[k]
  return temp_edges

def createEmbeddingFiles(raw_edges, treshold):
  print('Number of raw edges --> ' + str(len(raw_edges)))
  limited_edges = limitEdges(edges, treshold)
  print('Number of limited edges --> ' + str(len(limited_edges)))
  f_train = open('train_treshold'+str(treshold)+'.dat','w') 
  f_test = open('test_treshold'+str(treshold)+'.dat','w') 
  for k, v in list(limited_edges.items())[:]:
    if v > 1:
      if(k[2]):
        f_test.write('u'+str(k[0])+'\t'+'i'+str(k[1])+'\t'+str(v)+'\n')
      else:
        f_train.write('u'+str(k[0])+'\t'+'i'+str(k[1])+'\t'+str(v)+'\n')
  f_train.close()
  f_test.close()
  command = 'BiNE-master/model/train.py --train-data train_treshold'+str(treshold)+'.dat --test-data test_treshold'+str(treshold)+'.dat --lam 0.025 --max-iter 10 --maxT 12 --d 128 --model-name project --rec 1 --large 2 --vectors-u vectors_token_treshold'+str(treshold)+'.dat --vectors-v  vectors_item_treshold'+str(treshold)+'.dat'
  !python {command}

# Prepare bine
!wget www.mo-ghorbani.ir/BiNE-master.zip
!unzip BiNE-master.zip
!pip install datasketch

# createEmbeddingFiles(edges, 10)

def createVectorsDataFrame(treshold):
  f = open('vectors_token_treshold'+str(treshold)+'.dat', "r")
  lines = f.readlines()
  vecotors_u = []
  for line in lines[:]:
    s = line.split()
    temp = []
    temp.append(s[0][1:])
    temp.append(' '.join(s[1:]))
    vecotors_u.append(temp)

  df_token = pd.DataFrame(vecotors_u, columns = ['token_id', 'vector'])
  f = open('vectors_item_treshold'+str(treshold)+'.dat', "r")
  lines = f.readlines()
  vecotors_v = []
  for line in lines[:]:
    s = line.split()
    temp = []
    temp.append(s[0][1:])
    temp.append(' '.join(s[1:]))
    vecotors_v.append(temp)

  df_item = pd.DataFrame(vecotors_v, columns = ['item_id', 'vector'])
  return df_token, df_item

def method1Evaluation(sampled_data_for_test, df_token, df_item):
  queries_mean_vectors = getQueriesMeanVectors(sampled_data_for_test, df_token)
  matchList = getRelevantItemsMethod1(queries_mean_vectors, df_item)
  evaluateList(matchList, sampled_data_for_test)
  evaluateList(matchList, sampled_data_for_test, test_data= False)

def getQueriesMeanVectors(sampled_data_for_test, df_token):
  row_tokens_mean_vect = {}
  zero_row_tokens_mean_vect = {}
  for key, tokens in enumerate(sampled_data_for_test['searchstring.tokens'][:]):
    tokens = tokens.split(',')
    tokens_mean = np.zeros(128)
    zero_vec = True
    for token in tokens:
      try:
        token_vec = df_token.loc[df_token['token_id'] == str(int(token) + 1000000)]['vector'].tolist()[0].split()
        token_vec = [float(i) for i in token_vec]
        zero_vec = False
      except Exception as e:
        # print(str(e))
        continue
      tokens_mean = np.add(tokens_mean, token_vec)
    if zero_vec: zero_row_tokens_mean_vect.update({key:key})
    row_tokens_mean_vect.update({key: tokens_mean/len(tokens)})
  return row_tokens_mean_vect

def getRelevantItemsMethod1(queries_mean_vectors, df_item):
  match = {}
  for row_index in queries_mean_vectors:
    query_mean_vec = queries_mean_vectors[row_index]
    temp = {}
    for key, item in enumerate(df_item['vector'][:]):
      item_vec = item.split()
      item_vec = [float(i) for i in item_vec]
      cosimilarity = 1 - spatial.distance.cosine(query_mean_vec, item_vec)
      temp.update({df_item['item_id'][key]: cosimilarity})
    sorted_x = sorted(temp.items(), key=operator.itemgetter(1))
    sorted_x.reverse()

    l = []
    for item in sorted_x[:20]:
      l.append(item)
    match.update({row_index: l})
  return match

# Utilities functions

# Return array of string of item ids
def getItems(row_index, sampled_data_for_test):
  items = sampled_data_for_test['items'][row_index].split(',')
  items = [str(int(i)+2000000) for i in items]
  return items

# Return array of float
def getRowQueryMeanVect(row_index):
  return row_tokens_mean_vect[row_index]

# Return array of strings
# Returns items that predicted
def getMatchedItems(row_index, match):
  tmp = []
  for item in match[row_index]:
    tmp.append(item[0])
  return tmp

def RR(ranked_list, ground_list):

    for i in range(len(ranked_list)):
        id = ranked_list[i]
        if id in ground_list:
            return 1 / (i + 1.0)
    return 0

def nDCG(ranked_list, ground_truth):
    dcg = 0
    idcg = IDCG(len(ground_truth))
    for i in range(len(ranked_list)):
        id = ranked_list[i]
        if id not in ground_truth:
            continue
        rank = i+1
        dcg += 1/ math.log(rank+1, 2)
    return dcg / idcg

def IDCG(n):
    idcg = 0
    for i in range(n):
        idcg += 1 / math.log(i+2, 2)
    return idcg

def AP(ranked_list, ground_truth):
  hits, sum_precs = 0, 0.0
  for i in range(len(ranked_list)):
      id = ranked_list[i]
      if id in ground_truth:
          hits += 1
          sum_precs += hits / (i+1.0)
  if hits > 0:
      return sum_precs / len(ground_truth)
  else:
      return 0.0

def evaluateList(match, sampled_data_for_test, test_data= True):
  ap_list = []
  rr_list = []
  ndcg_list = []
  index_list = None
  info_text = None
  if test_data:
    index_list = list(sampled_data_for_test[sampled_data_for_test['is.test'] == True].index)
    info_text = 'Evaluation for test data:'
  else:
    index_list = list(sampled_data_for_test[sampled_data_for_test['is.test'] == False].index)
    info_text = 'Evaluation for train data:'
  for i in index_list:
    ranked_list = getMatchedItems(i, match)
    ground_list = getItems(i, sampled_data_for_test)
    ap_list.append(AP(ranked_list, ground_list))
    rr_list.append(RR(ranked_list, ground_list))
    ndcg_list.append(nDCG(ranked_list, ground_list))

  mapScore = 0
  mrrScore = 0
  mndcg = 0
  if len(ap_list) != 0:
    mapScore = sum(ap_list) / len(ap_list)
  if len(rr_list) != 0:
    mrrScore = sum(rr_list) / len(rr_list)
  if len(ndcg_list) != 0:
    mndcg = sum(ndcg_list) / len(ndcg_list)
  print(info_text)
  print("MAP: " + str(mapScore))
  print("MRR: "+str(mrrScore))
  print("NDCG: "+str(mndcg))

def method2_evaluation(sampled_data_for_test, df_token, df_item):
  matchList = getRelevantItemsMethod2(sampled_data_for_test, df_token, df_item)
  evaluateList(matchList, sampled_data_for_test)
  evaluateList(matchList, sampled_data_for_test, test_data= False)

def getRelevantItemsMethod2(sampled_data_for_test, df_token, df_item):
  match = {}
  # Iterate over sample table
  for key_row, tokens in enumerate(sampled_data_for_test['searchstring.tokens'][:]):
    temp = {}
    tokens = tokens.split(',')
    tokens = [int(i)+1000000 for i in tokens]
    for key, item in df_item.iterrows():
      item_vec = item['vector'].split()
      item_vec = [float(i) for i in item_vec]
      item_score = 0
      counter = 0
      for token in tokens:
        try:
          token_vec = df_token.loc[df_token['token_id'] == str(token)]['vector'].tolist()[0].split()
          token_vec = [float(i) for i in token_vec]
        except:
          continue
        item_score += (1 - spatial.distance.cosine(token_vec, item_vec))
        counter += 1
      if counter != 0:
        temp.update({item['item_id']: item_score/counter})
      else:
        temp.update({key: item_score})
    sorted_x = sorted(temp.items(), key=operator.itemgetter(1))
    sorted_x.reverse()
    match.update({key_row: sorted_x[:20]})
  return match

def projectEvaluation(trainingEdgeMinWeight, testSamplingFraction, method1=True, method2= True):
  df_token, df_item = createVectorsDataFrame(trainingEdgeMinWeight)
  sampled_data_for_test = sampled_data.sample(frac= testSamplingFraction, random_state=10).reset_index(drop=True)
  print('Edge minimum weight is: '+str(trainingEdgeMinWeight))
  print('Number of sampled data for test is: ' + str(sampled_data_for_test.shape[0]))
  if method1:
    print('\nMethod 1 Evaluation')
    method1Evaluation(sampled_data_for_test, df_token, df_item)
  if method2:
    print('\nMethod 2 Evaluation')
    method2_evaluation(sampled_data_for_test, df_token, df_item)
  return df_token, df_item

df_token, df_item = projectEvaluation(3, 0.001, method1= True, method2=False)

df_token, df_item = projectEvaluation(5, 0.001, method1= True, method2=False)

df_token, df_item = projectEvaluation(10, 0.001, method1= True, method2=False)

df_token, df_item = projectEvaluation(3, 0.0002, method1= False, method2=True)

df_token, df_item = projectEvaluation(5, 0.0002, method1= False, method2=True)

df_token, df_item = projectEvaluation(10, 0.0002, method1= False, method2=True)

# libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
 
# Data for method1 map
x = [3, 5, 10]
train_map_m1 = [0.003, 0.012, 0.006]
test_map_m1 = [0.022, 0.044, 0.034]
train_map_m2 = [0.000, 0.002, 0.002]
test_map_m2 = [0.020, 0.029, 0.022]
df=pd.DataFrame({'x': x, 'train_map_m1': train_map_m1, 'test_map_m1':test_map_m1, 'train_map_m2': train_map_m2, 'test_map_m2':test_map_m2})
plt.figure(figsize=(10,8))
plt.plot( 'x', 'train_map_m1', data=df, marker='o', markerfacecolor='#4840C5', markersize=6, color='#D3D5E5', linewidth=2)
plt.plot( 'x', 'train_map_m2', data=df, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=2)
plt.plot( 'x', 'test_map_m1', data=df, marker='o', markerfacecolor='red', markersize=6, color='#FFD2D2', linewidth=2)
plt.plot( 'x', 'test_map_m2', data=df, marker='o', markerfacecolor='#FC7D0E', markersize=6, color='#FFE7D2', linewidth=2)
plt.xlabel('Edge Weight Minimum')
plt.ylabel('MAP')
plt.title('Methods MAP Comparsion') 
plt.legend()
plt.show()

# Data for method1 mrr
x = [3, 5, 10]
train_mrr_m1 = [0.026, 0.092, 0.024]
test_mrr_m1 = [0.170, 0.333, 0.222]
train_mrr_m2 = [0.007, 0.037, 0.025]
test_mrr_m2 = [0.333, 0.333, 0.333]
df=pd.DataFrame({'x': x, 'train_mrr_m1': train_mrr_m1, 'test_mrr_m1':test_mrr_m1, 'train_mrr_m2': train_mrr_m2, 'test_mrr_m2':test_mrr_m2})
plt.figure(figsize=(10,8))
plt.plot( 'x', 'train_mrr_m1', data=df, marker='o', markerfacecolor='#4840C5', markersize=6, color='#D3D5E5', linewidth=2)
plt.plot( 'x', 'train_mrr_m2', data=df, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=2)
plt.plot( 'x', 'test_mrr_m1', data=df, marker='o', markerfacecolor='red', markersize=6, color='#FFD2D2', linewidth=2)
plt.plot( 'x', 'test_mrr_m2', data=df, marker='o', markerfacecolor='#FC7D0E', markersize=6, color='#FFE7D2', linewidth=2)
plt.xlabel('Edge Weight Minimum')
plt.ylabel('MRR')
plt.title('Methods MRR Comparisons') 
plt.legend()
plt.show()

